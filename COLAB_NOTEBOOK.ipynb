{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning Pipeline - Google Colab\n",
        "\n",
        "This notebook fine-tunes **LLaMA 3.1-8B** for emotional support and career guidance.\n",
        "\n",
        "**Important:** \n",
        "- Mount Google Drive for persistent storage\n",
        "- Set your Hugging Face token\n",
        "- Request access to `meta-llama/Llama-3.1-8B-Instruct` before training\n",
        "- **Requires Colab Pro** (16GB+ VRAM recommended)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup and Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive for persistent storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Set project root (adjust if your folder is named differently)\n",
        "project_root = \"/content/drive/MyDrive/llama-finetuning\"\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "Path(project_root).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Change to project directory\n",
        "os.chdir(project_root)\n",
        "sys.path.insert(0, project_root)\n",
        "\n",
        "print(f\"‚úì Working directory: {os.getcwd()}\")\n",
        "print(f\"‚úì Project files: {os.listdir('.')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install PyTorch with CUDA support\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Install other dependencies\n",
        "!pip install -q transformers datasets peft bitsandbytes accelerate huggingface-hub python-dotenv tensorboard\n",
        "\n",
        "print(\"‚úì Dependencies installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Configure Hugging Face Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set your Hugging Face token\n",
        "# Get it from: https://huggingface.co/settings/tokens\n",
        "os.environ['HF_TOKEN'] = 'your_token_here'  # ‚ö†Ô∏è REPLACE WITH YOUR TOKEN\n",
        "\n",
        "# Verify token is set\n",
        "if os.environ.get('HF_TOKEN') and os.environ['HF_TOKEN'] != 'your_token_here':\n",
        "    print(\"‚úì HF_TOKEN is set\")\n",
        "    from huggingface_hub import login\n",
        "    login(token=os.environ['HF_TOKEN'])\n",
        "    print(\"‚úì Logged in to Hugging Face\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  WARNING: Please set your HF_TOKEN before proceeding!\")\n",
        "    print(\"Get your token from: https://huggingface.co/settings/tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Verify GPU (Important for 8B!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    \n",
        "    print(f\"‚úì GPU: {gpu_name}\")\n",
        "    print(f\"‚úì VRAM: {gpu_memory_gb:.1f} GB\")\n",
        "    print()\n",
        "    \n",
        "    # Check if GPU is sufficient for 8B model\n",
        "    if gpu_memory_gb >= 70:\n",
        "        print(\"üöÄüöÄ EXTREME PERFORMANCE MODE - 80GB+ GPU DETECTED!\")\n",
        "        print(\"   Ultra-optimizations applied:\")\n",
        "        print(\"   - Massive batch size: 24 (vs 8 for A100 40GB, 2 for T4)\")\n",
        "        print(\"   - No gradient accumulation (batch is large enough)\")\n",
        "        print(\"   - bfloat16 precision (faster than fp16)\")\n",
        "        print(\"   - Maximum data loading (16 workers)\")\n",
        "        print(\"   - Gradient checkpointing DISABLED (for max speed)\")\n",
        "        print(\"   - Full sequence length (2048 tokens)\")\n",
        "        print(\"   - Faster optimizer (adamw_torch)\")\n",
        "        print()\n",
        "        print(\"   Expected training time: 1-2 hours ‚ö°‚ö°‚ö°\")\n",
        "        print(\"   (2-3x faster than standard A100 40GB!)\")\n",
        "    elif \"A100\" in gpu_name or gpu_memory_gb >= 40:\n",
        "        print(\"üöÄ A100 GPU DETECTED - High-Performance Mode Enabled!\")\n",
        "        print(\"   Optimizations applied:\")\n",
        "        print(\"   - Large batch size: 8 (vs 2 for T4/V100)\")\n",
        "        print(\"   - bfloat16 precision (faster than fp16)\")\n",
        "        print(\"   - Optimized data loading (4 workers)\")\n",
        "        print(\"   - Full sequence length (2048 tokens)\")\n",
        "        print(\"   - Faster optimizer (adamw_torch)\")\n",
        "        print()\n",
        "        print(\"   Expected training time: 2-4 hours (vs 4-10 hours on T4)\")\n",
        "    elif gpu_memory_gb >= 32:\n",
        "        print(\"‚úÖ Excellent! Your GPU has plenty of memory for 8B model.\")\n",
        "    elif gpu_memory_gb >= 16:\n",
        "        print(\"‚ö†Ô∏è  Your GPU should work, but may be tight. Monitor for OOM errors.\")\n",
        "        print(\"   Consider reducing batch size if you encounter memory issues.\")\n",
        "    else:\n",
        "        print(\"‚ùå WARNING: Your GPU may not have enough memory for 8B model!\")\n",
        "        print(\"   Minimum recommended: 16GB VRAM\")\n",
        "        print(\"   Consider using 1B model instead or upgrading to Colab Pro+\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: No GPU detected!\")\n",
        "    print(\"   Please enable GPU in Colab: Runtime ‚Üí Change runtime type ‚Üí GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Pre-Flight Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python scripts/preflight_check.py --model_size 8B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Prepare Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python scripts/prepare_data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Start Training (8B Model)\n",
        "\n",
        "**Training will take 4-10 hours depending on your GPU.**\n",
        "\n",
        "**Important:** Checkpoints are automatically saved to Google Drive every **100 steps** (reduced from 500 for better Colab stability). Training runs for **2 epochs** total."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python training/train.py --model_size 8B --output_dir ./outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 8: Evaluate Model (After Training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate test samples and evaluate model performance\n",
        "!python scripts/evaluate_model.py \\\n",
        "    --model_path ./outputs \\\n",
        "    --base_model_path meta-llama/Llama-3.1-8B-Instruct \\\n",
        "    --model_size 8B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Merge LoRA Adapters (Optional)\n",
        "\n",
        "Merge LoRA adapters with base model for easier deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge LoRA adapters with base model\n",
        "# This creates a single model file (easier to use, but larger ~16GB)\n",
        "!python export/merge_lora.py \\\n",
        "    --base_model_path meta-llama/Llama-3.1-8B-Instruct \\\n",
        "    --lora_adapter_path ./outputs \\\n",
        "    --output_path ./outputs/merged_model \\\n",
        "    --model_size 8B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Push to Hugging Face (Optional)\n",
        "\n",
        "Upload your model to Hugging Face Hub for sharing or backup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload model to Hugging Face Hub\n",
        "# Replace 'your-username/your-model-name' with your repository name\n",
        "!python export/push_to_huggingface.py \\\n",
        "    --model_path ./outputs/merged_model \\\n",
        "    --repo_id your-username/llama3.1-8b-emotional-career \\\n",
        "    --token $env:HF_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Monitor Training (Optional)\n",
        "\n",
        "Run these cells in separate tabs to monitor training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor GPU usage (updates every second)\n",
        "# Press Stop when done monitoring\n",
        "!nvidia-smi -l 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### View Training Logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View latest training log\n",
        "!tail -50 logs/training_*.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TensorBoard (Optional)\n",
        "\n",
        "Visualize training metrics in real-time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load TensorBoard extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Start TensorBoard\n",
        "%tensorboard --logdir ./outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resume Training (If Session Disconnects)\n",
        "\n",
        "If Colab disconnects, you can resume from the last checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resume from checkpoint\n",
        "# Replace 'checkpoint-1000' with your latest checkpoint number\n",
        "# Check available checkpoints: !ls outputs/\n",
        "\n",
        "!python training/train.py \\\n",
        "    --model_size 8B \\\n",
        "    --output_dir ./outputs \\\n",
        "    --resume_from_checkpoint ./outputs/checkpoint-1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check Available Checkpoints\n",
        "\n",
        "List all saved checkpoints to find the latest one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all checkpoints\n",
        "!ls -lh outputs/checkpoint-*/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting\n",
        "\n",
        "Common issues and solutions for 8B model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Out of Memory (OOM) Error\n",
        "\n",
        "If you get OOM errors during training:\n",
        "\n",
        "1. **Reduce batch size** - Edit `config/training_config.py`:\n",
        "   - Set `per_device_train_batch_size = 1`\n",
        "   - Increase `gradient_accumulation_steps = 16`\n",
        "\n",
        "2. **Reduce sequence length**:\n",
        "   - Set `max_seq_length = 1024` or `512`\n",
        "\n",
        "3. **Use smaller model**: Switch to 1B model instead\n",
        "\n",
        "4. **Upgrade GPU**: Get Colab Pro+ for A100 (40GB+ VRAM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Session Disconnected\n",
        "\n",
        "If Colab disconnects:\n",
        "- All checkpoints are saved to Google Drive automatically\n",
        "- Resume from last checkpoint using Step 11 (Resume Training)\n",
        "- Check available checkpoints with Step 12\n",
        "\n",
        "### Slow Training\n",
        "\n",
        "Training 8B model takes time:\n",
        "- **T4 (16GB)**: 6-10 hours\n",
        "- **V100 (32GB)**: 4-6 hours\n",
        "- **A100 (40GB+)**: 3-5 hours\n",
        "\n",
        "This is normal! Be patient and let it run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8B Model Configuration\n",
        "\n",
        "The pipeline **automatically detects your GPU** and optimizes settings:\n",
        "\n",
        "### For A100 (40GB+):\n",
        "- **Batch size**: 8 per device (4x faster!)\n",
        "- **Gradient accumulation**: 2 steps\n",
        "- **Effective batch size**: 16\n",
        "- **Sequence length**: 2048 tokens (full length)\n",
        "- **Precision**: bfloat16 (faster than fp16)\n",
        "- **Optimizer**: adamw_torch (faster)\n",
        "- **Data workers**: 4 (faster loading)\n",
        "- **LoRA rank**: 32, alpha: 64\n",
        "- **Learning rate**: 2e-4\n",
        "\n",
        "### For T4/V100 (16-32GB):\n",
        "- **Batch size**: 2 per device\n",
        "- **Gradient accumulation**: 8 steps\n",
        "- **Effective batch size**: 16\n",
        "- **Sequence length**: 1536 tokens\n",
        "- **Precision**: fp16\n",
        "- **LoRA rank**: 32, alpha: 64\n",
        "- **Learning rate**: 2e-4\n",
        "\n",
        "Settings are automatically optimized based on your GPU!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Expected Training Times\n",
        "\n",
        "- **T4 (16GB)**: 6-10 hours\n",
        "- **V100 (32GB)**: 4-6 hours\n",
        "- **A100 (40GB+)**: 2-4 hours ‚ö° (with high-performance optimizations)\n",
        "\n",
        "**Checkpoint Settings:**\n",
        "- Checkpoints save every **100 steps** to Google Drive (optimized for Colab stability)\n",
        "- Training runs for **2 epochs** total\n",
        "- Last 5 checkpoints are kept (increased from 3)\n",
        "\n",
        "**A100 Performance Boost:**\n",
        "- 4x larger batch size (8 vs 2) = faster training\n",
        "- bfloat16 precision = faster computation\n",
        "- Optimized data loading = less waiting\n",
        "- **Result: 2-4x faster training compared to T4!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected usage for 8B:**\n",
        "- T4 (16GB): ~14-15GB used (tight)\n",
        "- V100 (32GB): ~20-25GB used (comfortable)\n",
        "- A100 (40GB+): ~25-30GB used (plenty of headroom)\n",
        "\n",
        "### View Training Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Watch logs\n",
        "!tail -f logs/training_*.log"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
