{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning Pipeline - Google Colab\n",
        "\n",
        "This notebook fine-tunes **LLaMA 3.1-8B** for emotional support and career guidance.\n",
        "\n",
        "**Important:** \n",
        "- Mount Google Drive for persistent storage\n",
        "- Set your Hugging Face token\n",
        "- Request access to `meta-llama/Llama-3.1-8B-Instruct` before training\n",
        "- **Requires Colab Pro** (16GB+ VRAM recommended)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup and Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive for persistent storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Set project root (adjust if your folder is named differently)\n",
        "project_root = \"/content/drive/MyDrive/Career_guidance\"\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "Path(project_root).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Change to project directory\n",
        "os.chdir(project_root)\n",
        "sys.path.insert(0, project_root)\n",
        "\n",
        "print(f\"‚úì Working directory: {os.getcwd()}\")\n",
        "print(f\"‚úì Project files: {os.listdir('.')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install PyTorch with CUDA support\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Install other dependencies\n",
        "!pip install -q transformers datasets peft bitsandbytes accelerate huggingface-hub python-dotenv tensorboard\n",
        "\n",
        "print(\"‚úì Dependencies installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Configure Hugging Face Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set your Hugging Face token\n",
        "# Get it from: https://huggingface.co/settings/tokens\n",
        "os.environ['HF_TOKEN'] = 'your_token_here'  # ‚ö†Ô∏è REPLACE WITH YOUR TOKEN\n",
        "\n",
        "# Verify token is set\n",
        "if os.environ.get('HF_TOKEN') and os.environ['HF_TOKEN'] != 'your_token_here':\n",
        "    print(\"‚úì HF_TOKEN is set\")\n",
        "    from huggingface_hub import login\n",
        "    login(token=os.environ['HF_TOKEN'])\n",
        "    print(\"‚úì Logged in to Hugging Face\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  WARNING: Please set your HF_TOKEN before proceeding!\")\n",
        "    print(\"Get your token from: https://huggingface.co/settings/tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Verify GPU (Important for 8B!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    \n",
        "    print(f\"‚úì GPU: {gpu_name}\")\n",
        "    print(f\"‚úì VRAM: {gpu_memory_gb:.1f} GB\")\n",
        "    print()\n",
        "    \n",
        "    # Check if GPU is sufficient for 8B model\n",
        "    if gpu_memory_gb >= 32:\n",
        "        print(\"‚úÖ Excellent! Your GPU has plenty of memory for 8B model.\")\n",
        "    elif gpu_memory_gb >= 16:\n",
        "        print(\"‚ö†Ô∏è  Your GPU should work, but may be tight. Monitor for OOM errors.\")\n",
        "        print(\"   Consider reducing batch size if you encounter memory issues.\")\n",
        "    else:\n",
        "        print(\"‚ùå WARNING: Your GPU may not have enough memory for 8B model!\")\n",
        "        print(\"   Minimum recommended: 16GB VRAM\")\n",
        "        print(\"   Consider using 1B model instead or upgrading to Colab Pro+\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: No GPU detected!\")\n",
        "    print(\"   Please enable GPU in Colab: Runtime ‚Üí Change runtime type ‚Üí GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Pre-Flight Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python scripts/preflight_check.py --model_size 8B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Prepare Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python scripts/prepare_data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Start Training (8B Model)\n",
        "\n",
        "**Training will take 4-10 hours depending on your GPU.**\n",
        "\n",
        "Checkpoints are automatically saved to Google Drive every 500 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python training/train.py --model_size 8B --output_dir ./outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 8: Evaluate Model (After Training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate test samples and evaluate model performance\n",
        "!python scripts/evaluate_model.py \\\n",
        "    --model_path ./outputs \\\n",
        "    --base_model_path meta-llama/Llama-3.1-8B-Instruct \\\n",
        "    --model_size 8B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Merge LoRA Adapters (Optional)\n",
        "\n",
        "Merge LoRA adapters with base model for easier deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge LoRA adapters with base model\n",
        "# This creates a single model file (easier to use, but larger ~16GB)\n",
        "!python export/merge_lora.py \\\n",
        "    --base_model_path meta-llama/Llama-3.1-8B-Instruct \\\n",
        "    --lora_adapter_path ./outputs \\\n",
        "    --output_path ./outputs/merged_model \\\n",
        "    --model_size 8B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Push to Hugging Face (Optional)\n",
        "\n",
        "Upload your model to Hugging Face Hub for sharing or backup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload model to Hugging Face Hub\n",
        "# Replace 'your-username/your-model-name' with your repository name\n",
        "!python export/push_to_huggingface.py \\\n",
        "    --model_path ./outputs/merged_model \\\n",
        "    --repo_id your-username/llama3.1-8b-emotional-career \\\n",
        "    --token $env:HF_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Monitor Training (Optional)\n",
        "\n",
        "Run these cells in separate tabs to monitor training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor GPU usage (updates every second)\n",
        "# Press Stop when done monitoring\n",
        "!nvidia-smi -l 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### View Training Logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View latest training log\n",
        "!tail -50 logs/training_*.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TensorBoard (Optional)\n",
        "\n",
        "Visualize training metrics in real-time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load TensorBoard extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Start TensorBoard\n",
        "%tensorboard --logdir ./outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resume Training (If Session Disconnects)\n",
        "\n",
        "If Colab disconnects, you can resume from the last checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resume from checkpoint\n",
        "# Replace 'checkpoint-1000' with your latest checkpoint number\n",
        "# Check available checkpoints: !ls outputs/\n",
        "\n",
        "!python training/train.py \\\n",
        "    --model_size 8B \\\n",
        "    --output_dir ./outputs \\\n",
        "    --resume_from_checkpoint ./outputs/checkpoint-1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check Available Checkpoints\n",
        "\n",
        "List all saved checkpoints to find the latest one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all checkpoints\n",
        "!ls -lh outputs/checkpoint-*/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting\n",
        "\n",
        "Common issues and solutions for 8B model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Out of Memory (OOM) Error\n",
        "\n",
        "If you get OOM errors during training:\n",
        "\n",
        "1. **Reduce batch size** - Edit `config/training_config.py`:\n",
        "   - Set `per_device_train_batch_size = 1`\n",
        "   - Increase `gradient_accumulation_steps = 16`\n",
        "\n",
        "2. **Reduce sequence length**:\n",
        "   - Set `max_seq_length = 1024` or `512`\n",
        "\n",
        "3. **Use smaller model**: Switch to 1B model instead\n",
        "\n",
        "4. **Upgrade GPU**: Get Colab Pro+ for A100 (40GB+ VRAM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Session Disconnected\n",
        "\n",
        "If Colab disconnects:\n",
        "- All checkpoints are saved to Google Drive automatically\n",
        "- Resume from last checkpoint using Step 11 (Resume Training)\n",
        "- Check available checkpoints with Step 12\n",
        "\n",
        "### Slow Training\n",
        "\n",
        "Training 8B model takes time:\n",
        "- **T4 (16GB)**: 6-10 hours\n",
        "- **V100 (32GB)**: 4-6 hours\n",
        "- **A100 (40GB+)**: 3-5 hours\n",
        "\n",
        "This is normal! Be patient and let it run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8B Model Configuration\n",
        "\n",
        "The pipeline automatically configures for 8B:\n",
        "- **Batch size**: 2 per device\n",
        "- **Gradient accumulation**: 8 steps\n",
        "- **Effective batch size**: 16\n",
        "- **Sequence length**: 1536 tokens\n",
        "- **LoRA rank**: 32, alpha: 64\n",
        "- **Learning rate**: 2e-4\n",
        "\n",
        "These settings are optimized for Colab Pro GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Expected Training Times\n",
        "\n",
        "- **T4 (16GB)**: 6-10 hours\n",
        "- **V100 (32GB)**: 4-6 hours\n",
        "- **A100 (40GB+)**: 3-5 hours\n",
        "\n",
        "Checkpoints save every 500 steps to Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## File Locations\n",
        "\n",
        "All files are automatically saved to Google Drive:\n",
        "- **Checkpoints**: `outputs/checkpoint-XXX/` (~32MB each)\n",
        "- **Final model**: `outputs/adapter_model.safetensors` (~32MB)\n",
        "- **Merged model**: `outputs/merged_model/` (~16GB, after step 9)\n",
        "- **Logs**: `logs/training_*.log`\n",
        "- **Evaluations**: `outputs/eval_samples.jsonl`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Success Checklist\n",
        "\n",
        "After training completes, you should have:\n",
        "- [ ] Final LoRA adapters in `outputs/`\n",
        "- [ ] Training logs in `logs/`\n",
        "- [ ] Evaluation results (after step 8)\n",
        "- [ ] (Optional) Merged model (after step 9)\n",
        "- [ ] (Optional) Model on Hugging Face (after step 10)\n",
        "\n",
        "**Congratulations!** Your fine-tuned 8B model is ready! üéâ"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
